What exactly is Gradient Descent?

Gradient Descent is an optimization algorithm used to minimize the loss function and update model parameters in machine learning. There are three main types:

1ï¸âƒ£ Batch Gradient Descent (BGD) ğŸ“Œ Updates parameters using the entire dataset at each step.

ğŸ”¹ How it works:

Computes gradients over the entire dataset.

Updates weights only once per epoch (after all samples are processed).

âœ… Pros:
âœ” More stable updates, converges smoothly.

âœ” Suitable for small datasets.

âŒ Cons:

âŒ Slow for large datasets (expensive computations).

âŒ Can get stuck in local minima.

2ï¸âƒ£ Stochastic Gradient Descent (SGD) ğŸ“Œ Updates parameters after every single training example (one at a time).

ğŸ”¹ How it works:

Randomly selects one sample and updates weights immediately.

Faster but more noisy updates.

âœ… Pros: 
âœ” Works well for large datasets.

âœ” Can escape local minima due to randomness.

âŒ Cons:

âŒ High variance in updates (can oscillate).

âŒ Convergence is less stable.

3ï¸âƒ£ Mini-Batch Gradient Descent ğŸ“Œ Updates parameters using a small batch of training examples.

ğŸ”¹ How it works:

A compromise between Batch GD and SGD.

Divides data into mini-batches (e.g., 32 or 64 samples per batch). Computes gradients for each mini-batch and updates weights.

âœ… Pros: 
âœ” Faster than Batch GD and less noisy than SGD.

âœ” Efficient for large datasets.

âœ” Takes advantage of vectorized operations (faster computation).

âŒ Cons:

âŒ Requires hyperparameter tuning (choosing the batch size).

********************************************************

âœ” Batch GD: Best for small datasets, but slow for large ones.

âœ” SGD: Works well for big datasets, but updates can be unstable.

âœ” Mini-Batch GD: The best compromise between speed and stability.

************************************************
